WARNING:root:OSError while attempting to symlink the latest log directory
ERROR: You need to initialize the database. Please run `airflow db init`. Make sure the command is run using Airflow version 2.7.0.
ERROR: You need to initialize the database. Please run `airflow db init`. Make sure the command is run using Airflow version 2.7.0.
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2024-04-24T20:10:09.948+0530[0m] {[34mexecutor_loader.py:[0m117} INFO[0m - Loaded executor: SequentialExecutor[0m
[2024-04-24 20:10:09 +0530] [198355] [INFO] Starting gunicorn 22.0.0
[2024-04-24 20:10:09 +0530] [198355] [INFO] Listening at: http://[::]:8793 (198355)
[2024-04-24 20:10:09 +0530] [198355] [INFO] Using worker: sync
[2024-04-24 20:10:09 +0530] [198356] [INFO] Booting worker with pid: 198356
[2024-04-24 20:10:10 +0530] [198357] [INFO] Booting worker with pid: 198357
[[34m2024-04-24T20:10:10.178+0530[0m] {[34mscheduler_job_runner.py:[0m800} INFO[0m - Starting the scheduler[0m
[[34m2024-04-24T20:10:10.178+0530[0m] {[34mscheduler_job_runner.py:[0m807} INFO[0m - Processing each file at most -1 times[0m
[[34m2024-04-24T20:10:10.184+0530[0m] {[34mmanager.py:[0m166} INFO[0m - Launched DagFileProcessorManager with pid: 198358[0m
[[34m2024-04-24T20:10:10.185+0530[0m] {[34mscheduler_job_runner.py:[0m1588} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-04-24T20:10:10.189+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2024-04-24T20:10:10.205+0530] {manager.py:411} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2024-04-24 20:10:51 +0530] [198355] [INFO] Handling signal: int
[[34m2024-04-24T20:10:51.140+0530[0m] {[34mscheduler_job_runner.py:[0m248} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[2024-04-24 20:10:51 +0530] [198356] [INFO] Worker exiting (pid: 198356)
[2024-04-24 20:10:51 +0530] [198357] [INFO] Worker exiting (pid: 198357)
[2024-04-24 20:10:51 +0530] [198355] [INFO] Shutting down: Master
[[34m2024-04-24T20:10:52.152+0530[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending Signals.SIGTERM to group 198358. PIDs of all processes in the group: [198358][0m
[[34m2024-04-24T20:10:52.152+0530[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal Signals.SIGTERM to group 198358[0m
[[34m2024-04-24T20:10:52.365+0530[0m] {[34mprocess_utils.py:[0m79} INFO[0m - Process psutil.Process(pid=198358, status='terminated', exitcode=0, started='20:10:09') (198358) terminated with exit code 0[0m
[[34m2024-04-24T20:10:52.381+0530[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending Signals.SIGTERM to group 198358. PIDs of all processes in the group: [][0m
[[34m2024-04-24T20:10:52.381+0530[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal Signals.SIGTERM to group 198358[0m
[[34m2024-04-24T20:10:52.382+0530[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending the signal Signals.SIGTERM to process 198358 as process group is missing.[0m
[[34m2024-04-24T20:10:52.382+0530[0m] {[34mscheduler_job_runner.py:[0m876} INFO[0m - Exited execute loop[0m
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2024-04-24T20:13:10.164+0530[0m] {[34mexecutor_loader.py:[0m117} INFO[0m - Loaded executor: SequentialExecutor[0m
[2024-04-24 20:13:10 +0530] [199987] [INFO] Starting gunicorn 22.0.0
[2024-04-24 20:13:10 +0530] [199987] [INFO] Listening at: http://[::]:8793 (199987)
[2024-04-24 20:13:10 +0530] [199987] [INFO] Using worker: sync
[2024-04-24 20:13:10 +0530] [199988] [INFO] Booting worker with pid: 199988
[2024-04-24 20:13:10 +0530] [199989] [INFO] Booting worker with pid: 199989
[[34m2024-04-24T20:13:10.366+0530[0m] {[34mscheduler_job_runner.py:[0m800} INFO[0m - Starting the scheduler[0m
[[34m2024-04-24T20:13:10.367+0530[0m] {[34mscheduler_job_runner.py:[0m807} INFO[0m - Processing each file at most -1 times[0m
[[34m2024-04-24T20:13:10.372+0530[0m] {[34mmanager.py:[0m166} INFO[0m - Launched DagFileProcessorManager with pid: 199991[0m
[[34m2024-04-24T20:13:10.373+0530[0m] {[34mscheduler_job_runner.py:[0m1588} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-04-24T20:13:10.376+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2024-04-24T20:13:10.391+0530] {manager.py:411} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-04-24T20:13:16.252+0530[0m] {[34mscheduler_job_runner.py:[0m248} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[2024-04-24 20:13:16 +0530] [199987] [INFO] Handling signal: int
[2024-04-24 20:13:16 +0530] [199989] [INFO] Worker exiting (pid: 199989)
[2024-04-24 20:13:16 +0530] [199988] [INFO] Worker exiting (pid: 199988)
[2024-04-24 20:13:16 +0530] [199987] [INFO] Shutting down: Master
[[34m2024-04-24T20:13:17.261+0530[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending Signals.SIGTERM to group 199991. PIDs of all processes in the group: [199991][0m
[[34m2024-04-24T20:13:17.261+0530[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal Signals.SIGTERM to group 199991[0m
[[34m2024-04-24T20:13:17.433+0530[0m] {[34mprocess_utils.py:[0m79} INFO[0m - Process psutil.Process(pid=199991, status='terminated', exitcode=0, started='20:13:10') (199991) terminated with exit code 0[0m
[[34m2024-04-24T20:13:17.444+0530[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending Signals.SIGTERM to group 199991. PIDs of all processes in the group: [][0m
[[34m2024-04-24T20:13:17.444+0530[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal Signals.SIGTERM to group 199991[0m
[[34m2024-04-24T20:13:17.444+0530[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending the signal Signals.SIGTERM to process 199991 as process group is missing.[0m
[[34m2024-04-24T20:13:17.444+0530[0m] {[34mscheduler_job_runner.py:[0m876} INFO[0m - Exited execute loop[0m
Traceback (most recent call last):
  File "/home/skumar/miniconda3/envs/Neuron/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/airflow/__main__.py", line 60, in main
    args.func(args)
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/airflow/utils/cli.py", line 111, in wrapper
    check_and_run_migrations()
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/airflow/utils/db.py", line 841, in check_and_run_migrations
    source_heads = set(env.script.get_heads())
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/alembic/script/base.py", line 406, in get_heads
    return list(self.revision_map.heads)
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 1113, in __get__
    obj.__dict__[self.__name__] = result = self.fget(obj)
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/alembic/script/revision.py", line 137, in heads
    self._revision_map
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 1113, in __get__
    obj.__dict__[self.__name__] = result = self.fget(obj)
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/alembic/script/revision.py", line 191, in _revision_map
    for revision in self._generator():
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/alembic/script/base.py", line 148, in _load_revisions
    script = Script._from_filename(self, dir_name, filename)
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/alembic/script/base.py", line 1036, in _from_filename
    module = util.load_python_file(dir_, filename)
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/alembic/util/pyfiles.py", line 93, in load_python_file
    module = load_module_py(module_id, path)
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/alembic/util/pyfiles.py", line 109, in load_module_py
    spec.loader.exec_module(module)  # type: ignore
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/airflow/migrations/versions/0073_2_0_0_prefix_dag_permissions.py", line 27, in <module>
    from flask_appbuilder import SQLA
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/flask_appbuilder/__init__.py", line 5, in <module>
    from .api import ModelRestApi  # noqa: F401
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/flask_appbuilder/api/__init__.py", line 27, in <module>
    from marshmallow import Schema, ValidationError
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/marshmallow/__init__.py", line 14, in <module>
    from marshmallow.schema import Schema, SchemaOpts
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/marshmallow/schema.py", line 17, in <module>
    from marshmallow import base, fields as ma_fields, class_registry, types
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/marshmallow/fields.py", line 17, in <module>
    from marshmallow import validate, utils, class_registry, types
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/marshmallow/validate.py", line 223, in <module>
    class Email(Validator):
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/marshmallow/validate.py", line 238, in Email
    DOMAIN_REGEX = re.compile(
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/re.py", line 252, in compile
    return _compile(pattern, flags)
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/re.py", line 304, in _compile
    p = sre_compile.compile(pattern, flags)
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/sre_compile.py", line 768, in compile
    code = _code(p, flags)
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/sre_compile.py", line 607, in _code
    _compile(code, p.data, flags)
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/sre_compile.py", line 209, in _compile
    _compile(code, av, flags)
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/sre_compile.py", line 156, in _compile
    _compile(code, av[2], flags)
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/sre_compile.py", line 156, in _compile
    _compile(code, av[2], flags)
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/sre_compile.py", line 120, in _compile
    charset, hascased = _optimize_charset(av, iscased, tolower, fixes)
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/sre_compile.py", line 396, in _optimize_charset
    if chunk in comps:
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/skumar/miniconda3/envs/Neuron/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/airflow/__main__.py", line 60, in main
    args.func(args)
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/airflow/utils/cli.py", line 97, in wrapper
    cli_action_loggers.on_pre_execution(**metrics)
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/airflow/utils/cli_action_loggers.py", line 70, in on_pre_execution
    callback(**kwargs)
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/airflow/utils/cli_action_loggers.py", line 110, in default_action_log
    session.bulk_insert_mappings(
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/contextlib.py", line 120, in __exit__
    next(self.gen)
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/airflow/utils/session.py", line 37, in create_session
    session.commit()
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1454, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 839, in commit
    trans.commit()
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2469, in commit
    self._do_commit()
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2659, in _do_commit
    self._connection_commit_impl()
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2630, in _connection_commit_impl
    self.connection._commit_impl()
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1096, in _commit_impl
    self._handle_dbapi_exception(e, None, None, None, None)
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2138, in _handle_dbapi_exception
    util.raise_(exc_info[1], with_traceback=exc_info[2])
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 1094, in _commit_impl
    self.engine.dialect.do_commit(self.connection)
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 686, in do_commit
    dbapi_connection.commit()
KeyboardInterrupt
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2024-04-24T20:22:41.330+0530[0m] {[34mexecutor_loader.py:[0m117} INFO[0m - Loaded executor: SequentialExecutor[0m
[2024-04-24 20:22:41 +0530] [204207] [INFO] Starting gunicorn 22.0.0
[2024-04-24 20:22:41 +0530] [204207] [INFO] Listening at: http://[::]:8793 (204207)
[2024-04-24 20:22:41 +0530] [204207] [INFO] Using worker: sync
[2024-04-24 20:22:41 +0530] [204208] [INFO] Booting worker with pid: 204208
[2024-04-24 20:22:41 +0530] [204209] [INFO] Booting worker with pid: 204209
[[34m2024-04-24T20:22:41.535+0530[0m] {[34mscheduler_job_runner.py:[0m800} INFO[0m - Starting the scheduler[0m
[[34m2024-04-24T20:22:41.535+0530[0m] {[34mscheduler_job_runner.py:[0m807} INFO[0m - Processing each file at most -1 times[0m
[[34m2024-04-24T20:22:41.541+0530[0m] {[34mmanager.py:[0m166} INFO[0m - Launched DagFileProcessorManager with pid: 204211[0m
[[34m2024-04-24T20:22:41.542+0530[0m] {[34mscheduler_job_runner.py:[0m1588} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-04-24T20:22:41.544+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2024-04-24T20:22:41.559+0530] {manager.py:411} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2024-04-24 20:22:52 +0530] [204207] [INFO] Handling signal: winch
[2024-04-24 20:23:41 +0530] [204207] [INFO] Handling signal: int
[[34m2024-04-24T20:23:41.521+0530[0m] {[34mscheduler_job_runner.py:[0m248} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[2024-04-24 20:23:41 +0530] [204209] [INFO] Worker exiting (pid: 204209)
[2024-04-24 20:23:41 +0530] [204208] [INFO] Worker exiting (pid: 204208)
[2024-04-24 20:23:41 +0530] [204207] [INFO] Shutting down: Master
[[34m2024-04-24T20:23:42.534+0530[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending Signals.SIGTERM to group 204211. PIDs of all processes in the group: [204211][0m
[[34m2024-04-24T20:23:42.534+0530[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal Signals.SIGTERM to group 204211[0m
[[34m2024-04-24T20:23:42.706+0530[0m] {[34mprocess_utils.py:[0m79} INFO[0m - Process psutil.Process(pid=204211, status='terminated', exitcode=0, started='20:22:41') (204211) terminated with exit code 0[0m
[[34m2024-04-24T20:23:42.716+0530[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending Signals.SIGTERM to group 204211. PIDs of all processes in the group: [][0m
[[34m2024-04-24T20:23:42.717+0530[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal Signals.SIGTERM to group 204211[0m
[[34m2024-04-24T20:23:42.717+0530[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending the signal Signals.SIGTERM to process 204211 as process group is missing.[0m
[[34m2024-04-24T20:23:42.717+0530[0m] {[34mscheduler_job_runner.py:[0m876} INFO[0m - Exited execute loop[0m
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2024-04-24T20:23:51.456+0530[0m] {[34mexecutor_loader.py:[0m117} INFO[0m - Loaded executor: SequentialExecutor[0m
[2024-04-24 20:23:51 +0530] [205189] [INFO] Starting gunicorn 22.0.0
[2024-04-24 20:23:51 +0530] [205189] [INFO] Listening at: http://[::]:8793 (205189)
[2024-04-24 20:23:51 +0530] [205189] [INFO] Using worker: sync
[2024-04-24 20:23:51 +0530] [205190] [INFO] Booting worker with pid: 205190
[2024-04-24 20:23:51 +0530] [205191] [INFO] Booting worker with pid: 205191
[[34m2024-04-24T20:23:51.667+0530[0m] {[34mscheduler_job_runner.py:[0m800} INFO[0m - Starting the scheduler[0m
[[34m2024-04-24T20:23:51.667+0530[0m] {[34mscheduler_job_runner.py:[0m807} INFO[0m - Processing each file at most -1 times[0m
[[34m2024-04-24T20:23:51.673+0530[0m] {[34mmanager.py:[0m166} INFO[0m - Launched DagFileProcessorManager with pid: 205192[0m
[[34m2024-04-24T20:23:51.674+0530[0m] {[34mscheduler_job_runner.py:[0m1588} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-04-24T20:23:51.677+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2024-04-24T20:23:51.692+0530] {manager.py:411} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-04-24T20:24:01.044+0530[0m] {[34mscheduler_job_runner.py:[0m248} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[2024-04-24 20:24:01 +0530] [205189] [INFO] Handling signal: int
[2024-04-24 20:24:01 +0530] [205190] [INFO] Worker exiting (pid: 205190)
[2024-04-24 20:24:01 +0530] [205191] [INFO] Worker exiting (pid: 205191)
[2024-04-24 20:24:01 +0530] [205189] [INFO] Shutting down: Master
[[34m2024-04-24T20:24:02.056+0530[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending Signals.SIGTERM to group 205192. PIDs of all processes in the group: [205192][0m
[[34m2024-04-24T20:24:02.056+0530[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal Signals.SIGTERM to group 205192[0m
[[34m2024-04-24T20:24:02.229+0530[0m] {[34mprocess_utils.py:[0m79} INFO[0m - Process psutil.Process(pid=205192, status='terminated', exitcode=0, started='20:23:51') (205192) terminated with exit code 0[0m
[[34m2024-04-24T20:24:02.247+0530[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending Signals.SIGTERM to group 205192. PIDs of all processes in the group: [][0m
[[34m2024-04-24T20:24:02.248+0530[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal Signals.SIGTERM to group 205192[0m
[[34m2024-04-24T20:24:02.248+0530[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending the signal Signals.SIGTERM to process 205192 as process group is missing.[0m
[[34m2024-04-24T20:24:02.248+0530[0m] {[34mscheduler_job_runner.py:[0m876} INFO[0m - Exited execute loop[0m
/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/airflow/configuration.py:751 UserWarning: Config scheduler.max_tis_per_query (value: 512) should NOT be greater than core.parallelism (value: 32). Will now use core.parallelism as the max task instances per query instead of specified value.
/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/airflow/cli/cli_config.py:974 DeprecationWarning: The namespace option in [kubernetes] has been moved to the namespace option in [kubernetes_executor] - the old setting has been used, but please update your config.
ERROR: You need to upgrade the database. Please run `airflow db upgrade`. Make sure the command is run using Airflow version 2.7.0.
/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/airflow/configuration.py:751 UserWarning: Config scheduler.max_tis_per_query (value: 512) should NOT be greater than core.parallelism (value: 32). Will now use core.parallelism as the max task instances per query instead of specified value.
/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/airflow/cli/cli_config.py:974 DeprecationWarning: The namespace option in [kubernetes] has been moved to the namespace option in [kubernetes_executor] - the old setting has been used, but please update your config.
ERROR: You need to upgrade the database. Please run `airflow db upgrade`. Make sure the command is run using Airflow version 2.7.0.
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2024-04-24T20:30:28.279+0530[0m] {[34mexecutor_loader.py:[0m117} INFO[0m - Loaded executor: SequentialExecutor[0m
[2024-04-24 20:30:28 +0530] [208356] [INFO] Starting gunicorn 22.0.0
[2024-04-24 20:30:28 +0530] [208356] [INFO] Listening at: http://[::]:8793 (208356)
[2024-04-24 20:30:28 +0530] [208356] [INFO] Using worker: sync
[2024-04-24 20:30:28 +0530] [208357] [INFO] Booting worker with pid: 208357
[2024-04-24 20:30:28 +0530] [208358] [INFO] Booting worker with pid: 208358
[[34m2024-04-24T20:30:28.477+0530[0m] {[34mscheduler_job_runner.py:[0m800} INFO[0m - Starting the scheduler[0m
[[34m2024-04-24T20:30:28.478+0530[0m] {[34mscheduler_job_runner.py:[0m807} INFO[0m - Processing each file at most -1 times[0m
[[34m2024-04-24T20:30:28.483+0530[0m] {[34mmanager.py:[0m166} INFO[0m - Launched DagFileProcessorManager with pid: 208359[0m
[[34m2024-04-24T20:30:28.485+0530[0m] {[34mscheduler_job_runner.py:[0m1588} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-04-24T20:30:28.488+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2024-04-24T20:30:28.503+0530] {manager.py:411} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[2024-04-24 20:31:05 +0530] [208356] [INFO] Handling signal: int
[[34m2024-04-24T20:31:05.745+0530[0m] {[34mscheduler_job_runner.py:[0m248} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[2024-04-24 20:31:05 +0530] [208357] [INFO] Worker exiting (pid: 208357)
[2024-04-24 20:31:05 +0530] [208358] [INFO] Worker exiting (pid: 208358)
[2024-04-24 20:31:05 +0530] [208356] [INFO] Shutting down: Master
[[34m2024-04-24T20:31:06.758+0530[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending Signals.SIGTERM to group 208359. PIDs of all processes in the group: [208359][0m
[[34m2024-04-24T20:31:06.758+0530[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal Signals.SIGTERM to group 208359[0m
[[34m2024-04-24T20:31:06.891+0530[0m] {[34mprocess_utils.py:[0m79} INFO[0m - Process psutil.Process(pid=208359, status='terminated', exitcode=0, started='20:30:28') (208359) terminated with exit code 0[0m
[[34m2024-04-24T20:31:06.900+0530[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending Signals.SIGTERM to group 208359. PIDs of all processes in the group: [][0m
[[34m2024-04-24T20:31:06.900+0530[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal Signals.SIGTERM to group 208359[0m
[[34m2024-04-24T20:31:06.900+0530[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending the signal Signals.SIGTERM to process 208359 as process group is missing.[0m
[[34m2024-04-24T20:31:06.901+0530[0m] {[34mscheduler_job_runner.py:[0m876} INFO[0m - Exited execute loop[0m
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2024-04-24T20:35:53.229+0530[0m] {[34mexecutor_loader.py:[0m117} INFO[0m - Loaded executor: SequentialExecutor[0m
[2024-04-24 20:35:53 +0530] [210980] [INFO] Starting gunicorn 22.0.0
[2024-04-24 20:35:53 +0530] [210980] [INFO] Listening at: http://[::]:8793 (210980)
[2024-04-24 20:35:53 +0530] [210980] [INFO] Using worker: sync
[2024-04-24 20:35:53 +0530] [210981] [INFO] Booting worker with pid: 210981
[2024-04-24 20:35:53 +0530] [210982] [INFO] Booting worker with pid: 210982
[[34m2024-04-24T20:35:53.436+0530[0m] {[34mscheduler_job_runner.py:[0m800} INFO[0m - Starting the scheduler[0m
[[34m2024-04-24T20:35:53.436+0530[0m] {[34mscheduler_job_runner.py:[0m807} INFO[0m - Processing each file at most -1 times[0m
[[34m2024-04-24T20:35:53.441+0530[0m] {[34mmanager.py:[0m166} INFO[0m - Launched DagFileProcessorManager with pid: 210984[0m
[[34m2024-04-24T20:35:53.443+0530[0m] {[34mscheduler_job_runner.py:[0m1588} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-04-24T20:35:53.445+0530[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2024-04-24T20:35:53.461+0530] {manager.py:411} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-04-24T20:37:34.852+0530[0m] {[34mdag.py:[0m3677} INFO[0m - Setting next_dagrun for Gem_training_pipeline to 2024-04-24T15:00:00+00:00, run_after=2024-04-24T16:00:00+00:00[0m
[[34m2024-04-24T20:37:35.618+0530[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 2 tasks up for execution:
	<TaskInstance: Gem_training_pipeline.training_ingestion scheduled__2024-04-24T14:00:00+00:00 [scheduled]>
	<TaskInstance: Gem_training_pipeline.training_ingestion manual__2024-04-24T15:07:31.167543+00:00 [scheduled]>[0m
[[34m2024-04-24T20:37:35.619+0530[0m] {[34mscheduler_job_runner.py:[0m479} INFO[0m - DAG Gem_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-04-24T20:37:35.619+0530[0m] {[34mscheduler_job_runner.py:[0m479} INFO[0m - DAG Gem_training_pipeline has 1/16 running and queued tasks[0m
[[34m2024-04-24T20:37:35.619+0530[0m] {[34mscheduler_job_runner.py:[0m595} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: Gem_training_pipeline.training_ingestion scheduled__2024-04-24T14:00:00+00:00 [scheduled]>
	<TaskInstance: Gem_training_pipeline.training_ingestion manual__2024-04-24T15:07:31.167543+00:00 [scheduled]>[0m
[[34m2024-04-24T20:37:35.620+0530[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task training_ingestion because previous state change time has not been saved[0m
[[34m2024-04-24T20:37:35.620+0530[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task training_ingestion because previous state change time has not been saved[0m
[[34m2024-04-24T20:37:35.620+0530[0m] {[34mscheduler_job_runner.py:[0m638} INFO[0m - Sending TaskInstanceKey(dag_id='Gem_training_pipeline', task_id='training_ingestion', run_id='scheduled__2024-04-24T14:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-04-24T20:37:35.620+0530[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'Gem_training_pipeline', 'training_ingestion', 'scheduled__2024-04-24T14:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-04-24T20:37:35.621+0530[0m] {[34mscheduler_job_runner.py:[0m638} INFO[0m - Sending TaskInstanceKey(dag_id='Gem_training_pipeline', task_id='training_ingestion', run_id='manual__2024-04-24T15:07:31.167543+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-04-24T20:37:35.621+0530[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'Gem_training_pipeline', 'training_ingestion', 'manual__2024-04-24T15:07:31.167543+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-04-24T20:37:35.763+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'Gem_training_pipeline', 'training_ingestion', 'scheduled__2024-04-24T14:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-04-24T20:37:36.866+0530[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /home/skumar/DataScience/Projects/Neuron/airflow/dags/training_pipeline.py[0m
[[34m2024-04-24T20:37:37.874+0530[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-04-24T20:37:38.094+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-04-24T20:37:38.094+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
Changing /home/skumar/DataScience/Projects/Neuron/airflow/logs/dag_id=Gem_training_pipeline/run_id=scheduled__2024-04-24T14:00:00+00:00/task_id=training_ingestion permission to 509
[[34m2024-04-24T20:37:38.125+0530[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: Gem_training_pipeline.training_ingestion scheduled__2024-04-24T14:00:00+00:00 [queued]> on host skumar[0m
[[34m2024-04-24T20:37:44.950+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'Gem_training_pipeline', 'training_ingestion', 'manual__2024-04-24T15:07:31.167543+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-04-24T20:37:45.644+0530[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /home/skumar/DataScience/Projects/Neuron/airflow/dags/training_pipeline.py[0m
[[34m2024-04-24T20:37:46.657+0530[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-04-24T20:37:46.880+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-04-24T20:37:46.881+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
Changing /home/skumar/DataScience/Projects/Neuron/airflow/logs/dag_id=Gem_training_pipeline/run_id=manual__2024-04-24T15:07:31.167543+00:00/task_id=training_ingestion permission to 509
[[34m2024-04-24T20:37:46.911+0530[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: Gem_training_pipeline.training_ingestion manual__2024-04-24T15:07:31.167543+00:00 [queued]> on host skumar[0m
[[34m2024-04-24T20:37:52.076+0530[0m] {[34mscheduler_job_runner.py:[0m688} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='Gem_training_pipeline', task_id='training_ingestion', run_id='scheduled__2024-04-24T14:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-04-24T20:37:52.076+0530[0m] {[34mscheduler_job_runner.py:[0m688} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='Gem_training_pipeline', task_id='training_ingestion', run_id='manual__2024-04-24T15:07:31.167543+00:00', try_number=1, map_index=-1)[0m
[[34m2024-04-24T20:37:52.084+0530[0m] {[34mscheduler_job_runner.py:[0m725} INFO[0m - TaskInstance Finished: dag_id=Gem_training_pipeline, task_id=training_ingestion, run_id=manual__2024-04-24T15:07:31.167543+00:00, map_index=-1, run_start_date=2024-04-24 15:07:47.013261+00:00, run_end_date=2024-04-24 15:07:51.324891+00:00, run_duration=4.31163, state=success, executor_state=success, try_number=1, max_tries=2, job_id=8, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-04-24 15:07:35.619443+00:00, queued_by_job_id=6, pid=212680[0m
[[34m2024-04-24T20:37:52.084+0530[0m] {[34mscheduler_job_runner.py:[0m725} INFO[0m - TaskInstance Finished: dag_id=Gem_training_pipeline, task_id=training_ingestion, run_id=scheduled__2024-04-24T14:00:00+00:00, map_index=-1, run_start_date=2024-04-24 15:07:38.218573+00:00, run_end_date=2024-04-24 15:07:44.261967+00:00, run_duration=6.043394, state=success, executor_state=success, try_number=1, max_tries=2, job_id=7, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-04-24 15:07:35.619443+00:00, queued_by_job_id=6, pid=212523[0m
[[34m2024-04-24T20:37:52.502+0530[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 2 tasks up for execution:
	<TaskInstance: Gem_training_pipeline.training_tranformation scheduled__2024-04-24T14:00:00+00:00 [scheduled]>
	<TaskInstance: Gem_training_pipeline.training_tranformation manual__2024-04-24T15:07:31.167543+00:00 [scheduled]>[0m
[[34m2024-04-24T20:37:52.502+0530[0m] {[34mscheduler_job_runner.py:[0m479} INFO[0m - DAG Gem_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-04-24T20:37:52.502+0530[0m] {[34mscheduler_job_runner.py:[0m479} INFO[0m - DAG Gem_training_pipeline has 1/16 running and queued tasks[0m
[[34m2024-04-24T20:37:52.502+0530[0m] {[34mscheduler_job_runner.py:[0m595} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: Gem_training_pipeline.training_tranformation scheduled__2024-04-24T14:00:00+00:00 [scheduled]>
	<TaskInstance: Gem_training_pipeline.training_tranformation manual__2024-04-24T15:07:31.167543+00:00 [scheduled]>[0m
[[34m2024-04-24T20:37:52.503+0530[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task training_tranformation because previous state change time has not been saved[0m
[[34m2024-04-24T20:37:52.503+0530[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task training_tranformation because previous state change time has not been saved[0m
[[34m2024-04-24T20:37:52.503+0530[0m] {[34mscheduler_job_runner.py:[0m638} INFO[0m - Sending TaskInstanceKey(dag_id='Gem_training_pipeline', task_id='training_tranformation', run_id='scheduled__2024-04-24T14:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-04-24T20:37:52.503+0530[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'Gem_training_pipeline', 'training_tranformation', 'scheduled__2024-04-24T14:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-04-24T20:37:52.504+0530[0m] {[34mscheduler_job_runner.py:[0m638} INFO[0m - Sending TaskInstanceKey(dag_id='Gem_training_pipeline', task_id='training_tranformation', run_id='manual__2024-04-24T15:07:31.167543+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-04-24T20:37:52.504+0530[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'Gem_training_pipeline', 'training_tranformation', 'manual__2024-04-24T15:07:31.167543+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-04-24T20:37:53.036+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'Gem_training_pipeline', 'training_tranformation', 'scheduled__2024-04-24T14:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-04-24T20:37:53.711+0530[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /home/skumar/DataScience/Projects/Neuron/airflow/dags/training_pipeline.py[0m
[[34m2024-04-24T20:37:54.712+0530[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-04-24T20:37:54.936+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-04-24T20:37:54.936+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
Changing /home/skumar/DataScience/Projects/Neuron/airflow/logs/dag_id=Gem_training_pipeline/run_id=scheduled__2024-04-24T14:00:00+00:00/task_id=training_tranformation permission to 509
[[34m2024-04-24T20:37:54.967+0530[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: Gem_training_pipeline.training_tranformation scheduled__2024-04-24T14:00:00+00:00 [queued]> on host skumar[0m
[[34m2024-04-24T20:37:56.661+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'Gem_training_pipeline', 'training_tranformation', 'manual__2024-04-24T15:07:31.167543+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-04-24T20:37:57.364+0530[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /home/skumar/DataScience/Projects/Neuron/airflow/dags/training_pipeline.py[0m
[[34m2024-04-24T20:37:58.407+0530[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-04-24T20:37:58.630+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-04-24T20:37:58.630+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
Changing /home/skumar/DataScience/Projects/Neuron/airflow/logs/dag_id=Gem_training_pipeline/run_id=manual__2024-04-24T15:07:31.167543+00:00/task_id=training_tranformation permission to 509
[[34m2024-04-24T20:37:58.661+0530[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: Gem_training_pipeline.training_tranformation manual__2024-04-24T15:07:31.167543+00:00 [queued]> on host skumar[0m
[[34m2024-04-24T20:38:00.560+0530[0m] {[34mscheduler_job_runner.py:[0m688} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='Gem_training_pipeline', task_id='training_tranformation', run_id='scheduled__2024-04-24T14:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-04-24T20:38:00.561+0530[0m] {[34mscheduler_job_runner.py:[0m688} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='Gem_training_pipeline', task_id='training_tranformation', run_id='manual__2024-04-24T15:07:31.167543+00:00', try_number=1, map_index=-1)[0m
[[34m2024-04-24T20:38:00.570+0530[0m] {[34mscheduler_job_runner.py:[0m725} INFO[0m - TaskInstance Finished: dag_id=Gem_training_pipeline, task_id=training_tranformation, run_id=manual__2024-04-24T15:07:31.167543+00:00, map_index=-1, run_start_date=2024-04-24 15:07:58.768176+00:00, run_end_date=2024-04-24 15:07:59.706396+00:00, run_duration=0.93822, state=success, executor_state=success, try_number=1, max_tries=2, job_id=10, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-04-24 15:07:52.502856+00:00, queued_by_job_id=6, pid=212938[0m
[[34m2024-04-24T20:38:00.570+0530[0m] {[34mscheduler_job_runner.py:[0m725} INFO[0m - TaskInstance Finished: dag_id=Gem_training_pipeline, task_id=training_tranformation, run_id=scheduled__2024-04-24T14:00:00+00:00, map_index=-1, run_start_date=2024-04-24 15:07:55.067105+00:00, run_end_date=2024-04-24 15:07:55.856401+00:00, run_duration=0.789296, state=success, executor_state=success, try_number=1, max_tries=2, job_id=9, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-04-24 15:07:52.502856+00:00, queued_by_job_id=6, pid=212845[0m
[[34m2024-04-24T20:38:01.080+0530[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 2 tasks up for execution:
	<TaskInstance: Gem_training_pipeline.training_model scheduled__2024-04-24T14:00:00+00:00 [scheduled]>
	<TaskInstance: Gem_training_pipeline.training_model manual__2024-04-24T15:07:31.167543+00:00 [scheduled]>[0m
[[34m2024-04-24T20:38:01.081+0530[0m] {[34mscheduler_job_runner.py:[0m479} INFO[0m - DAG Gem_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-04-24T20:38:01.081+0530[0m] {[34mscheduler_job_runner.py:[0m479} INFO[0m - DAG Gem_training_pipeline has 1/16 running and queued tasks[0m
[[34m2024-04-24T20:38:01.081+0530[0m] {[34mscheduler_job_runner.py:[0m595} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: Gem_training_pipeline.training_model scheduled__2024-04-24T14:00:00+00:00 [scheduled]>
	<TaskInstance: Gem_training_pipeline.training_model manual__2024-04-24T15:07:31.167543+00:00 [scheduled]>[0m
[[34m2024-04-24T20:38:01.083+0530[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task training_model because previous state change time has not been saved[0m
[[34m2024-04-24T20:38:01.083+0530[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task training_model because previous state change time has not been saved[0m
[[34m2024-04-24T20:38:01.084+0530[0m] {[34mscheduler_job_runner.py:[0m638} INFO[0m - Sending TaskInstanceKey(dag_id='Gem_training_pipeline', task_id='training_model', run_id='scheduled__2024-04-24T14:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-04-24T20:38:01.084+0530[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'Gem_training_pipeline', 'training_model', 'scheduled__2024-04-24T14:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-04-24T20:38:01.084+0530[0m] {[34mscheduler_job_runner.py:[0m638} INFO[0m - Sending TaskInstanceKey(dag_id='Gem_training_pipeline', task_id='training_model', run_id='manual__2024-04-24T15:07:31.167543+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-04-24T20:38:01.085+0530[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'Gem_training_pipeline', 'training_model', 'manual__2024-04-24T15:07:31.167543+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-04-24T20:38:01.165+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'Gem_training_pipeline', 'training_model', 'scheduled__2024-04-24T14:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-04-24T20:38:01.964+0530[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /home/skumar/DataScience/Projects/Neuron/airflow/dags/training_pipeline.py[0m
[[34m2024-04-24T20:38:02.973+0530[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-04-24T20:38:03.194+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-04-24T20:38:03.195+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
Changing /home/skumar/DataScience/Projects/Neuron/airflow/logs/dag_id=Gem_training_pipeline/run_id=scheduled__2024-04-24T14:00:00+00:00/task_id=training_model permission to 509
[[34m2024-04-24T20:38:03.226+0530[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: Gem_training_pipeline.training_model scheduled__2024-04-24T14:00:00+00:00 [queued]> on host skumar[0m
[[34m2024-04-24T20:38:04.781+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'Gem_training_pipeline', 'training_model', 'manual__2024-04-24T15:07:31.167543+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-04-24T20:38:05.439+0530[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /home/skumar/DataScience/Projects/Neuron/airflow/dags/training_pipeline.py[0m
[[34m2024-04-24T20:38:06.432+0530[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-04-24T20:38:06.650+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-04-24T20:38:06.650+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
Changing /home/skumar/DataScience/Projects/Neuron/airflow/logs/dag_id=Gem_training_pipeline/run_id=manual__2024-04-24T15:07:31.167543+00:00/task_id=training_model permission to 509
[[34m2024-04-24T20:38:06.680+0530[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: Gem_training_pipeline.training_model manual__2024-04-24T15:07:31.167543+00:00 [queued]> on host skumar[0m
[[34m2024-04-24T20:38:08.275+0530[0m] {[34mscheduler_job_runner.py:[0m688} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='Gem_training_pipeline', task_id='training_model', run_id='scheduled__2024-04-24T14:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-04-24T20:38:08.276+0530[0m] {[34mscheduler_job_runner.py:[0m688} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='Gem_training_pipeline', task_id='training_model', run_id='manual__2024-04-24T15:07:31.167543+00:00', try_number=1, map_index=-1)[0m
[[34m2024-04-24T20:38:08.283+0530[0m] {[34mscheduler_job_runner.py:[0m725} INFO[0m - TaskInstance Finished: dag_id=Gem_training_pipeline, task_id=training_model, run_id=manual__2024-04-24T15:07:31.167543+00:00, map_index=-1, run_start_date=2024-04-24 15:08:06.772571+00:00, run_end_date=2024-04-24 15:08:07.674765+00:00, run_duration=0.902194, state=success, executor_state=success, try_number=1, max_tries=2, job_id=12, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-04-24 15:08:01.082175+00:00, queued_by_job_id=6, pid=213123[0m
[[34m2024-04-24T20:38:08.284+0530[0m] {[34mscheduler_job_runner.py:[0m725} INFO[0m - TaskInstance Finished: dag_id=Gem_training_pipeline, task_id=training_model, run_id=scheduled__2024-04-24T14:00:00+00:00, map_index=-1, run_start_date=2024-04-24 15:08:03.359842+00:00, run_end_date=2024-04-24 15:08:04.028975+00:00, run_duration=0.669133, state=success, executor_state=success, try_number=1, max_tries=2, job_id=11, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-04-24 15:08:01.082175+00:00, queued_by_job_id=6, pid=213027[0m
[[34m2024-04-24T20:38:08.593+0530[0m] {[34mdagrun.py:[0m659} INFO[0m - Marking run <DagRun Gem_training_pipeline @ 2024-04-24 14:00:00+00:00: scheduled__2024-04-24T14:00:00+00:00, state:running, queued_at: 2024-04-24 15:07:33.711485+00:00. externally triggered: False> successful[0m
[[34m2024-04-24T20:38:08.594+0530[0m] {[34mdagrun.py:[0m710} INFO[0m - DagRun Finished: dag_id=Gem_training_pipeline, execution_date=2024-04-24 14:00:00+00:00, run_id=scheduled__2024-04-24T14:00:00+00:00, run_start_date=2024-04-24 15:07:35.080200+00:00, run_end_date=2024-04-24 15:08:08.593968+00:00, run_duration=33.513768, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2024-04-24 14:00:00+00:00, data_interval_end=2024-04-24 15:00:00+00:00, dag_hash=f998774e2bf3afc3ae2b53f040cb1290[0m
[[34m2024-04-24T20:38:08.598+0530[0m] {[34mdag.py:[0m3677} INFO[0m - Setting next_dagrun for Gem_training_pipeline to 2024-04-24T15:00:00+00:00, run_after=2024-04-24T16:00:00+00:00[0m
[[34m2024-04-24T20:38:08.600+0530[0m] {[34mdagrun.py:[0m659} INFO[0m - Marking run <DagRun Gem_training_pipeline @ 2024-04-24 15:07:31.167543+00:00: manual__2024-04-24T15:07:31.167543+00:00, state:running, queued_at: 2024-04-24 15:07:31.242588+00:00. externally triggered: True> successful[0m
[[34m2024-04-24T20:38:08.600+0530[0m] {[34mdagrun.py:[0m710} INFO[0m - DagRun Finished: dag_id=Gem_training_pipeline, execution_date=2024-04-24 15:07:31.167543+00:00, run_id=manual__2024-04-24T15:07:31.167543+00:00, run_start_date=2024-04-24 15:07:35.080454+00:00, run_end_date=2024-04-24 15:08:08.600799+00:00, run_duration=33.520345, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-04-24 14:00:00+00:00, data_interval_end=2024-04-24 15:00:00+00:00, dag_hash=f998774e2bf3afc3ae2b53f040cb1290[0m
[[34m2024-04-24T20:38:08.603+0530[0m] {[34mdag.py:[0m3677} INFO[0m - Setting next_dagrun for Gem_training_pipeline to 2024-04-24T15:00:00+00:00, run_after=2024-04-24T16:00:00+00:00[0m
[[34m2024-04-24T20:40:54.211+0530[0m] {[34mscheduler_job_runner.py:[0m1588} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-04-24T20:41:03.492+0530[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: Gem_training_pipeline.training_ingestion manual__2024-04-24T15:11:02.704603+00:00 [scheduled]>[0m
[[34m2024-04-24T20:41:03.492+0530[0m] {[34mscheduler_job_runner.py:[0m479} INFO[0m - DAG Gem_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-04-24T20:41:03.492+0530[0m] {[34mscheduler_job_runner.py:[0m595} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: Gem_training_pipeline.training_ingestion manual__2024-04-24T15:11:02.704603+00:00 [scheduled]>[0m
[[34m2024-04-24T20:41:03.494+0530[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task training_ingestion because previous state change time has not been saved[0m
[[34m2024-04-24T20:41:03.494+0530[0m] {[34mscheduler_job_runner.py:[0m638} INFO[0m - Sending TaskInstanceKey(dag_id='Gem_training_pipeline', task_id='training_ingestion', run_id='manual__2024-04-24T15:11:02.704603+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-04-24T20:41:03.494+0530[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'Gem_training_pipeline', 'training_ingestion', 'manual__2024-04-24T15:11:02.704603+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-04-24T20:41:03.569+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'Gem_training_pipeline', 'training_ingestion', 'manual__2024-04-24T15:11:02.704603+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-04-24T20:41:04.265+0530[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /home/skumar/DataScience/Projects/Neuron/airflow/dags/training_pipeline.py[0m
[[34m2024-04-24T20:41:05.259+0530[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-04-24T20:41:07.495+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-04-24T20:41:07.495+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
Changing /home/skumar/DataScience/Projects/Neuron/airflow/logs/dag_id=Gem_training_pipeline/run_id=manual__2024-04-24T15:11:02.704603+00:00/task_id=training_ingestion permission to 509
[[34m2024-04-24T20:41:07.889+0530[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: Gem_training_pipeline.training_ingestion manual__2024-04-24T15:11:02.704603+00:00 [queued]> on host skumar[0m
[[34m2024-04-24T20:41:14.213+0530[0m] {[34mscheduler_job_runner.py:[0m688} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='Gem_training_pipeline', task_id='training_ingestion', run_id='manual__2024-04-24T15:11:02.704603+00:00', try_number=1, map_index=-1)[0m
[[34m2024-04-24T20:41:14.220+0530[0m] {[34mscheduler_job_runner.py:[0m725} INFO[0m - TaskInstance Finished: dag_id=Gem_training_pipeline, task_id=training_ingestion, run_id=manual__2024-04-24T15:11:02.704603+00:00, map_index=-1, run_start_date=2024-04-24 15:11:08.497105+00:00, run_end_date=2024-04-24 15:11:13.505437+00:00, run_duration=5.008332, state=success, executor_state=success, try_number=1, max_tries=2, job_id=13, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-04-24 15:11:03.493206+00:00, queued_by_job_id=6, pid=215248[0m
[[34m2024-04-24T20:41:14.630+0530[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: Gem_training_pipeline.training_tranformation manual__2024-04-24T15:11:02.704603+00:00 [scheduled]>[0m
[[34m2024-04-24T20:41:14.631+0530[0m] {[34mscheduler_job_runner.py:[0m479} INFO[0m - DAG Gem_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-04-24T20:41:14.631+0530[0m] {[34mscheduler_job_runner.py:[0m595} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: Gem_training_pipeline.training_tranformation manual__2024-04-24T15:11:02.704603+00:00 [scheduled]>[0m
[[34m2024-04-24T20:41:14.632+0530[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task training_tranformation because previous state change time has not been saved[0m
[[34m2024-04-24T20:41:14.632+0530[0m] {[34mscheduler_job_runner.py:[0m638} INFO[0m - Sending TaskInstanceKey(dag_id='Gem_training_pipeline', task_id='training_tranformation', run_id='manual__2024-04-24T15:11:02.704603+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-04-24T20:41:14.632+0530[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'Gem_training_pipeline', 'training_tranformation', 'manual__2024-04-24T15:11:02.704603+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-04-24T20:41:14.708+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'Gem_training_pipeline', 'training_tranformation', 'manual__2024-04-24T15:11:02.704603+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-04-24T20:41:15.733+0530[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /home/skumar/DataScience/Projects/Neuron/airflow/dags/training_pipeline.py[0m
[[34m2024-04-24T20:41:16.724+0530[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-04-24T20:41:16.942+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-04-24T20:41:16.942+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
Changing /home/skumar/DataScience/Projects/Neuron/airflow/logs/dag_id=Gem_training_pipeline/run_id=manual__2024-04-24T15:11:02.704603+00:00/task_id=training_tranformation permission to 509
[[34m2024-04-24T20:41:16.973+0530[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: Gem_training_pipeline.training_tranformation manual__2024-04-24T15:11:02.704603+00:00 [queued]> on host skumar[0m
[[34m2024-04-24T20:41:20.412+0530[0m] {[34mscheduler_job_runner.py:[0m688} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='Gem_training_pipeline', task_id='training_tranformation', run_id='manual__2024-04-24T15:11:02.704603+00:00', try_number=1, map_index=-1)[0m
[[34m2024-04-24T20:41:20.420+0530[0m] {[34mscheduler_job_runner.py:[0m725} INFO[0m - TaskInstance Finished: dag_id=Gem_training_pipeline, task_id=training_tranformation, run_id=manual__2024-04-24T15:11:02.704603+00:00, map_index=-1, run_start_date=2024-04-24 15:11:17.064019+00:00, run_end_date=2024-04-24 15:11:19.331511+00:00, run_duration=2.267492, state=success, executor_state=success, try_number=1, max_tries=2, job_id=14, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-04-24 15:11:14.631632+00:00, queued_by_job_id=6, pid=215419[0m
[[34m2024-04-24T20:41:20.877+0530[0m] {[34mscheduler_job_runner.py:[0m414} INFO[0m - 1 tasks up for execution:
	<TaskInstance: Gem_training_pipeline.training_model manual__2024-04-24T15:11:02.704603+00:00 [scheduled]>[0m
[[34m2024-04-24T20:41:20.877+0530[0m] {[34mscheduler_job_runner.py:[0m479} INFO[0m - DAG Gem_training_pipeline has 0/16 running and queued tasks[0m
[[34m2024-04-24T20:41:20.877+0530[0m] {[34mscheduler_job_runner.py:[0m595} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: Gem_training_pipeline.training_model manual__2024-04-24T15:11:02.704603+00:00 [scheduled]>[0m
[[34m2024-04-24T20:41:20.957+0530[0m] {[34mtaskinstance.py:[0m1441} WARNING[0m - cannot record scheduled_duration for task training_model because previous state change time has not been saved[0m
[[34m2024-04-24T20:41:20.958+0530[0m] {[34mscheduler_job_runner.py:[0m638} INFO[0m - Sending TaskInstanceKey(dag_id='Gem_training_pipeline', task_id='training_model', run_id='manual__2024-04-24T15:11:02.704603+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-04-24T20:41:20.958+0530[0m] {[34mbase_executor.py:[0m144} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'Gem_training_pipeline', 'training_model', 'manual__2024-04-24T15:11:02.704603+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-04-24T20:41:21.027+0530[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'Gem_training_pipeline', 'training_model', 'manual__2024-04-24T15:11:02.704603+00:00', '--local', '--subdir', 'DAGS_FOLDER/training_pipeline.py'][0m
[[34m2024-04-24T20:41:21.679+0530[0m] {[34mdagbag.py:[0m539} INFO[0m - Filling up the DagBag from /home/skumar/DataScience/Projects/Neuron/airflow/dags/training_pipeline.py[0m
[[34m2024-04-24T20:41:22.664+0530[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2024-04-24T20:41:22.883+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/skumar/miniconda3/envs/Neuron/lib/python3.8/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2024-04-24T20:41:22.883+0530[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
Changing /home/skumar/DataScience/Projects/Neuron/airflow/logs/dag_id=Gem_training_pipeline/run_id=manual__2024-04-24T15:11:02.704603+00:00/task_id=training_model permission to 509
[[34m2024-04-24T20:41:22.915+0530[0m] {[34mtask_command.py:[0m415} INFO[0m - Running <TaskInstance: Gem_training_pipeline.training_model manual__2024-04-24T15:11:02.704603+00:00 [queued]> on host skumar[0m
[[34m2024-04-24T20:41:24.273+0530[0m] {[34mscheduler_job_runner.py:[0m688} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='Gem_training_pipeline', task_id='training_model', run_id='manual__2024-04-24T15:11:02.704603+00:00', try_number=1, map_index=-1)[0m
[[34m2024-04-24T20:41:24.282+0530[0m] {[34mscheduler_job_runner.py:[0m725} INFO[0m - TaskInstance Finished: dag_id=Gem_training_pipeline, task_id=training_model, run_id=manual__2024-04-24T15:11:02.704603+00:00, map_index=-1, run_start_date=2024-04-24 15:11:23.021826+00:00, run_end_date=2024-04-24 15:11:23.686908+00:00, run_duration=0.665082, state=success, executor_state=success, try_number=1, max_tries=2, job_id=15, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-04-24 15:11:20.878100+00:00, queued_by_job_id=6, pid=215527[0m
[[34m2024-04-24T20:41:24.521+0530[0m] {[34mdagrun.py:[0m659} INFO[0m - Marking run <DagRun Gem_training_pipeline @ 2024-04-24 15:11:02.704603+00:00: manual__2024-04-24T15:11:02.704603+00:00, state:running, queued_at: 2024-04-24 15:11:02.713545+00:00. externally triggered: True> successful[0m
[[34m2024-04-24T20:41:24.521+0530[0m] {[34mdagrun.py:[0m710} INFO[0m - DagRun Finished: dag_id=Gem_training_pipeline, execution_date=2024-04-24 15:11:02.704603+00:00, run_id=manual__2024-04-24T15:11:02.704603+00:00, run_start_date=2024-04-24 15:11:03.155318+00:00, run_end_date=2024-04-24 15:11:24.521822+00:00, run_duration=21.366504, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-04-24 14:00:00+00:00, data_interval_end=2024-04-24 15:00:00+00:00, dag_hash=f998774e2bf3afc3ae2b53f040cb1290[0m
[[34m2024-04-24T20:41:24.525+0530[0m] {[34mdag.py:[0m3677} INFO[0m - Setting next_dagrun for Gem_training_pipeline to 2024-04-24T15:00:00+00:00, run_after=2024-04-24T16:00:00+00:00[0m
[[34m2024-04-24T20:45:54.955+0530[0m] {[34mscheduler_job_runner.py:[0m1588} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-04-24T20:50:55.200+0530[0m] {[34mscheduler_job_runner.py:[0m1588} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-04-24T20:55:56.245+0530[0m] {[34mscheduler_job_runner.py:[0m1588} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-04-24T21:00:56.485+0530[0m] {[34mscheduler_job_runner.py:[0m1588} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-04-24T21:05:58.208+0530[0m] {[34mscheduler_job_runner.py:[0m1588} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-04-24T21:10:59.552+0530[0m] {[34mscheduler_job_runner.py:[0m1588} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[2024-04-24 21:14:01 +0530] [210980] [INFO] Handling signal: int
[2024-04-24 21:14:01 +0530] [210982] [INFO] Worker exiting (pid: 210982)
[2024-04-24 21:14:01 +0530] [210981] [INFO] Worker exiting (pid: 210981)
[2024-04-24 21:14:01 +0530] [210980] [INFO] Shutting down: Master
[[34m2024-04-24T21:14:01.888+0530[0m] {[34mscheduler_job_runner.py:[0m248} INFO[0m - Exiting gracefully upon receiving signal 2[0m
[[34m2024-04-24T21:14:02.897+0530[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending Signals.SIGTERM to group 210984. PIDs of all processes in the group: [210984][0m
[[34m2024-04-24T21:14:02.898+0530[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal Signals.SIGTERM to group 210984[0m
[[34m2024-04-24T21:14:03.070+0530[0m] {[34mprocess_utils.py:[0m79} INFO[0m - Process psutil.Process(pid=210984, status='terminated', exitcode=0, started='20:35:53') (210984) terminated with exit code 0[0m
[[34m2024-04-24T21:14:03.088+0530[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending Signals.SIGTERM to group 210984. PIDs of all processes in the group: [][0m
[[34m2024-04-24T21:14:03.088+0530[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal Signals.SIGTERM to group 210984[0m
[[34m2024-04-24T21:14:03.089+0530[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending the signal Signals.SIGTERM to process 210984 as process group is missing.[0m
[[34m2024-04-24T21:14:03.089+0530[0m] {[34mscheduler_job_runner.py:[0m876} INFO[0m - Exited execute loop[0m
